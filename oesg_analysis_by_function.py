# this program does OESG analysis:
# average and std dev for sites
# box plots of sites for ipcc_regions
# Mann Kendall trends
# pick what this program does by turning various functions on and off


import numpy as np
from datetime import datetime as dt
import calendar
import glob
import pandas as pd
#matplotlib.use('Agg')
import matplotlib.pyplot as plt
plt.ion()
import psycopg2, psycopg2.extras
import gc, re
import pymannkendall as mk
from sqlalchemy import create_engine, text
import geopandas as gp
import os
import matplotlib.dates as md

# local module import
from credentials import sql_engine_string_generator


################## SQL database stuff ##############################

# create the sql engine from a string generated by the string generator
sql_engine_string = sql_engine_string_generator('QP_SERVER','QP_HGEE_USER','QP_HGEE_PASSWORD','hgee')
sql_engine = create_engine(sql_engine_string)

################### common data IO elements ##########################

# import the ipcc_regions as a dataframe
sql_data_query = """
                select distinct on (ipcc_region) ipcc_region from sites order by ipcc_region;
                """
with sql_engine.connect() as conn:
    ipcc_analysis_df = pd.read_sql_query(sql_data_query, conn)


# initialize the dataframe for ipcc mann_kendall results
ipcc_analysis_df.set_index('ipcc_region', drop=True, inplace=True) # set the ipcc_regions as the index
ipcc_analysis_df['mk_slope']=np.nan # add a blank list for the MK results

ipcc_regions_list = ipcc_analysis_df.index.to_list()
ipcc_regions_list = ['N.W.North-America']#,'N.E.North-America','W.North-America'] # a temporary list for testing

# initialize a dataframe for monthly averages 
ipcc_master_df=pd.DataFrame() # create an empty dataframe to house each concentration query by ipcc_region column

# grab the sites list from the sites table
sql_data_query = """
                SELECT CONCAT( sites.site, ' (', sites.country_code, ')') AS site_name_full
                from sites
                where ipcc_region = '{}'
                order by site_name_full
                """
with sql_engine.connect() as conn:
    sites_df = pd.read_sql_query(sql_data_query, conn)


################## averaging function for all sites ################

# do the global average
def global_site_average(sql_engine):
    sql_data_query = """
                    SET TIME ZONE 'GMT';
                    SELECT CONCAT( subq.site, ' (', sites.country_code, ')') AS site_name_full
                        , subq.avg_concentration
                        , subq.stdev_concentration
                    FROM (
                        SELECT site ,avg(concentration)::float AS avg_concentration ,stddev(concentration)::float AS stdev_concentration 
                        FROM all__hgee_v2 GROUP BY site
                        ) AS subq, sites
                        WHERE sites.site = subq.site
                        ORDER BY sites.site;
                    """

    with sql_engine.connect() as conn:
    # create the dataframes from the sql query
        mercury_df = pd.read_sql_query(sql_data_query, conn)
    
    mercury_df_sorted=mercury_df.sort_values(by='avg_concentration')
    
    # plot the means and std's
    plt.figure(figsize=(80, 10))
    plt.errorbar(mercury_df_sorted['site_name_full'], mercury_df_sorted['avg_concentration'], yerr=mercury_df_sorted['stdev_concentration'], fmt='o', capsize=5, capthick=2, elinewidth=1.5, markersize=4)
    plt.xticks(rotation=90)
    # ipcc_site_mean_df.plot.scatter('site', 'mean', yerr='std', capsize=3, title=('Mean site TGM/GEM for '+ipcc_region),xlabel='site',ylabel='Concentration (ngm$\mathregular{^3}$)',rot=90)
    plt.title('Mean site TGM/GEM For All sites ')
    plt.xlabel('site')
    plt.ylabel('Concentration (ngm$\mathregular{^-3}$)')
    plt.ylim(-10,100)
    plt.tight_layout()
    plt.savefig('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Results and Plots/mean_concentration_global.png')
    plt.show(block=True)

################## averaging function by ipcc_region #########################

def ipcc_site_average(sql_engine,ipcc_regions_list):
    
    # for ipcc_region in ipcc_analysis_df['ipcc_region']:
    for ipcc_region in ipcc_regions_list:#ipcc_analysis_df.index:
        print (ipcc_region)

        #define a sql query to grab the concentration from a particular site for a particular species from 2010 on
        sql_data_query = """
                    SET TIME ZONE 'GMT';
                    SELECT CONCAT( subq.site, ' (', sites.country_code, ')') AS site_name_full
                        , subq.avg_concentration
                        , subq.stdev_concentration
                    FROM (
                        SELECT site ,avg(concentration)::float AS avg_concentration ,stddev(concentration)::float AS stdev_concentration 
                        FROM all__hgee_v2 GROUP BY site
                        ) AS subq, sites
                        WHERE sites.site = subq.site
                        and ipcc_region = '{}'
                        ORDER BY sites.site;
                """.format(ipcc_region)    

        with sql_engine.connect() as conn:
        # create the dataframes from the sql query
            mercury_df = pd.read_sql_query(sql_data_query, conn)

    if mercury_df.shape[0]!=0:
        # assign the country code to the dataframe

        # plot the means and std's
        plt.figure(figsize=(20, 10))
        plt.errorbar(mercury_df['site_name_full'], mercury_df['avg_concentration'], yerr=mercury_df['stdev_concentration'], fmt='o', capsize=5, capthick=2, elinewidth=1.5, markersize=4)
        plt.xticks(rotation=90)
        # ipcc_site_mean_df.plot.scatter('site', 'mean', yerr='std', capsize=3, title=('Mean site TGM/GEM for '+ipcc_region),xlabel='site',ylabel='Concentration (ngm$\mathregular{^3}$)',rot=90)
        plt.title('Mean site TGM/GEM For '+ipcc_region)
        plt.xlabel('site')
        plt.ylabel('Concentration (ngm$\mathregular{^-3}$)')
        plt.ylim(-10,100)
        plt.tight_layout()
        plt.savefig('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Results and Plots/mean_concentration_'+ipcc_region+'.png')
        plt.show(block=True)

############################# monthly mean function for trend analysis and such ##########

def monthly_mean(sql_engine,site_list):
        # for ipcc_region in ipcc_analysis_df['ipcc_region']:
    for site in site_list:#ipcc_analysis_df.index:
        print (site)

        #define a sql query to grab the concentration from a particular site for a particular species from 2010 on
        sql_data_query = """
                    SET TIME ZONE 'GMT';
                    select datetime, concentration from all__hgee_v2
                    where species in ['TGM','GEM'] and site  = '{}'
                    order by datetime
                """.format(site)    

        with sql_engine.connect() as conn:
        # create the dataframes from the sql query
            mercury_df = pd.read_sql_query(sql_data_query, conn)
            month_list=[] # initialize a blank month list
            for i in range(1,13):
                month_list.append(calendar.month_name[i][:3])

            # create a blank dataframe with 12 column months and 31 calendar days as the index
            master_monthly_df=pd.DataFrame(columns=month_list)

            for i, month in enumerate(month_list):
                print (month)
                datetime_month=i+1
                # print (mercury_df['datetime'].dt.month==datetime_month)
                monthly_df_index = mercury_df.loc[mercury_df['datetime'].dt.month==datetime_month].index
                # print (mercury_df.loc[monthly_df_index,'concentration'].reset_index(drop=True))
                master_monthly_df[month]=mercury_df.loc[monthly_df_index,'concentration'].reset_index(drop=True)

        # plot the means and std's
        plt.figure(figsize=(20, 10))
        master_monthly_df.boxplot()    
        plt.title('site TGM/GEM Boxplot For '+ipcc_region)
        plt.xlabel('Month')
        plt.ylabel('Concentration (ngm$\mathregular{^-3}$)')
        plt.tight_layout()
        plt.savefig('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Results and Plots/monthly_box_plot_'+site+'.png')
        plt.show(block=True)

############################## extraction of frequency information from the minamata format files ##############
def site_frequency_test():
    # initialize a frequency dataframe for tracking those with frequency information
    frequency_df = pd.DataFrame(columns=['frequency'])
    frequency_df.index.name = 'site'
    folders='\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Data/*'
    folder_list=glob.glob(folders)
    folder_list.remove('\\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Data\\Passives') # grab all the folders 
    # print (folder_list)
    for folder in folder_list: # for each folder grab all the files
        print (folder)
        folder_path=folder+'/air_data/minamata_format/*.csv'
        file_list=glob.glob(folder_path)        
        for file in file_list: # for each file in the list
            if len(file)>260:
                print (len(file))
                print (file)
                continue
            file_metadata_df=pd.read_csv(file,index_col=0,nrows=60,usecols=range(3))
            # print (file_metadata_df.columns)
            site_name=file_metadata_df.loc['*site IDENTIFICATION','Unnamed: 1']
            data_frequency=file_metadata_df.loc['*SAMPLING FREQUENCY OF DATA IN THIS FILE','Unnamed: 1']
            # check to see if that site is already included
            if site_name not in frequency_df.index:
                frequency_df.loc[site_name,'frequency']=data_frequency
    print (frequency_df)
    frequency_df.to_csv('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Results and Plots/frequency_info.csv')

########################### function to insert frequency data into the table ###################################
def freq_insert(sql_engine):
    # load the csv file into a dataframe
    freq_df=pd.read_csv('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Results and Plots/all_sites_frequency.csv')
    print (freq_df.head)
    # Connect and execute update per row
    with sql_engine.begin() as conn:
        for _, row in freq_df.iterrows():
            print (row["site"],row["frequency"])
            conn.execute(
                text("""
                    UPDATE sites
                    SET freq = :frequency
                    WHERE site = :site  
                """),
                {"site": row["site"], "frequency": row["frequency"]}
            )

def coverage_calculator(sql_engine):
    # define an sql query that grabs the duration from the hgee_active table and frequency from the sites table
    sql_data_query = """
                        SELECT 
                            s.site, 
                            s.freq, 
                            h.min_dt, 
                            h.max_dt,
                            h.count
                        FROM sites s
                        JOIN (
                            SELECT 
                                site, 
                                MIN(datetime) AS min_dt, 
                                MAX(datetime) AS max_dt,
                                COUNT(*) AS count
                            FROM hgee_active
                            WHERE matrix = 'air'
                            GROUP BY site
                        ) h ON s.site = h.site
                        WHERE h.min_dt > '2000-01-01'
                        AND h.max_dt > '2015-01-01';        """
                            
    
    with sql_engine.begin() as conn:
        # put the results into a dataframe
        freq_df = pd.read_sql_query(sql_data_query, conn)

    freq_df['freq'] = pd.to_timedelta(freq_df['freq'])
    freq_df['perfect_count'] = (freq_df['max_dt'] - freq_df['min_dt']) / freq_df['freq']
    freq_df['percent_coverage'] = freq_df['count']/freq_df['perfect_count']*100
    print(freq_df.loc[freq_df['percent_coverage'] < 75, ['site', 'percent_coverage']])
    # print (freq_df.loc[:,['site', 'percent_coverage']])
    
############################ gantt chart for data availability ###################################
def gantt_plotter(sql_engine):
    # set the psql query
    sql_data_query = """
                set time zone GMT;
                select distinct on (site) site, country, min(datetime) as start_dt, max(datetime) as end_dt,species from hgee_active where datetime > '2000-01-01' GROUP BY (site,country,species); 
            """

    with sql_engine.connect() as conn:
    # create the dataframes from the sql query
        gantt_mercury_tracking_dataframe = pd.read_sql_query(sql_data_query, conn)

    
    # grab the passives (for now) from teh big passives file
    passives_gantt_tracker_df=pd.read_csv('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Data/Passives/data_files/global_passives.csv',usecols=[1,2,3,4])

    passives_gantt_tracker_df['start_dt']=pd.to_datetime(passives_gantt_tracker_df['start_dt'])
    passives_gantt_tracker_df['end_dt']=pd.to_datetime(passives_gantt_tracker_df['end_dt'])

    # Create new rows to add to the Gantt tracking dataframe
    passive_rows = []

    for site in passives_gantt_tracker_df['site'].unique():
        site_df = passives_gantt_tracker_df[passives_gantt_tracker_df['site'] == site]
        start_date = site_df['start_dt'].min()
        end_date = site_df['end_dt'].max()
        country = site_df['country'].iloc[0]  # Extract single value
        passive_species = 'TPM'

        # only use samples that spanned more than one 'season'
        if len(site_df['start_dt'])>1:
            passive_rows.append({
                'site': site,
                'country': country,
                'start_dt': start_date,
                'end_dt': end_date,
                'species': passive_species
            })

    # Create a DataFrame from the new rows and append to the existing Gantt dataframe
    passive_df = pd.DataFrame(passive_rows)
    gantt_mercury_tracking_dataframe = pd.concat([gantt_mercury_tracking_dataframe, passive_df], ignore_index=True)

    # concatenate the site and country to one lable
    gantt_mercury_tracking_dataframe['site'] = gantt_mercury_tracking_dataframe['site']+' '+gantt_mercury_tracking_dataframe['country']
    gantt_mercury_tracking_dataframe.drop(columns=['country'], inplace=True)
    
    # set the site as the index
    gantt_mercury_tracking_dataframe.set_index('site',drop=True, inplace=True)

    
    # loop through each species
    species_list=gantt_mercury_tracking_dataframe['species'].unique()
    for species in species_list:
        print (species)
        sub_df = gantt_mercury_tracking_dataframe[gantt_mercury_tracking_dataframe['species'] == species]
        print (sub_df.index)

        # sort the df by end_dt
        sub_df.sort_values(by=['end_dt'], inplace=True)        
        
        if species=='PBM':
            fig_size=(15,5)
        elif species=='GEM':
            fig_size=(15,15)
        elif species=='GOM':
            fig_size=(15,5)
        elif species=='total_mercury':
            fig_size=(15,30)
        elif species=='TGM':
            fig_size=(15,15)
        elif species=='TPM':
            fig_size=(15,40)                

        fig,ax = plt.subplots(figsize=fig_size)
        ax.xaxis_date()
        plt.hlines(sub_df.index, md.date2num(sub_df['start_dt']), md.date2num(sub_df['end_dt']), )
        plt.xticks(fontsize=20)
        plt.xlabel('Date',fontsize=20)
        plt.ylabel('Station',fontsize=20)
        plt.title('Mercury Measurement Time Span by Station for: '+species,fontsize=20)
        plt.grid(axis="x")
        # plt.legend(labels=subset_df.index)
        # plt.show()
        fig.tight_layout()
        plt.savefig('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Results and Plots/gantt-plot_'+species+'.png')
        plt.close()
   






# # run the global average
# global_site_average(sql_engine)

# # run the ipcc average
# ipcc_site_average(sql_engine,ipcc_regions_list)

# run the ipcc monthly box plot
# monthly_mean(sql_engine,ipcc_regions_list)

# run the frequency test
#site_frequency_test()

# run the frequency table insert
# freq_insert(sql_engine)

# run the coverage calculator
coverage_calculator(sql_engine)

# run the gantt plotter
# gantt_plotter(sql_engine)
