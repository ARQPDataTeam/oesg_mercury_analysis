# this program does OESG analysis:
# average and std dev for sites
# box plots of sites for ipcc_regions
# Mann Kendall trends
# pick what this program does by turning various functions on and off


import numpy as np
from datetime import datetime as dt
import calendar
import glob
import pandas as pd
#matplotlib.use('Agg')
import matplotlib.pyplot as plt
plt.ion()
import pymannkendall as mk
from sqlalchemy import create_engine, text
import geopandas as gp
import matplotlib.dates as md
from shapely import Point
from shapely.geometry import box 
import matplotlib as mpl
from scipy import stats


# local module import
from credentials import sql_engine_string_generator

################## SQL database stuff ##############################

# create the sql engine from a string generated by the string generator
sql_engine_string = sql_engine_string_generator('QP_SERVER','QP_HGEE_USER','QP_HGEE_PASSWORD','hgee')
sql_engine = create_engine(sql_engine_string)

################### common data IO elements ##########################

# import the ipcc_regions as a dataframe
sql_data_query = """
                select distinct on (ipcc_region) ipcc_region from sites order by ipcc_region;
                """
with sql_engine.connect() as conn:
    ipcc_analysis_df = pd.read_sql_query(sql_data_query, conn)


# initialize the dataframe for ipcc mann_kendall results
ipcc_analysis_df.set_index('ipcc_region', drop=True, inplace=True) # set the ipcc_regions as the index
ipcc_analysis_df['mk_slope']=np.nan # add a blank list for the MK results

ipcc_regions_list = ipcc_analysis_df.index.to_list()
ipcc_regions_list = ['N.W.North-America']#,'N.E.North-America','W.North-America'] # a temporary list for testing

# initialize a dataframe for monthly averages 
ipcc_master_df=pd.DataFrame() # create an empty dataframe to house each concentration query by ipcc_region column

# grab the sites list from the sites table
sql_data_query = """
                SELECT CONCAT( sites.site, ' (', sites.country_code, ')') AS site_name_full
                from sites
                where ipcc_region = '{}'
                order by site_name_full
                """
with sql_engine.connect() as conn:
    sites_df = pd.read_sql_query(sql_data_query, conn)


################## averaging function for all sites ################

# do the global average
def global_site_average(sql_engine):
    sql_data_query = """
                    SET TIME ZONE 'GMT';
                    SELECT CONCAT( subq.site, ' (', sites.country_code, ')') AS site_name_full
                        , subq.avg_concentration
                        , subq.stdev_concentration
                    FROM (
                        SELECT site ,avg(concentration)::float AS avg_concentration ,stddev(concentration)::float AS stdev_concentration 
                        FROM all__hgee_v2 GROUP BY site
                        ) AS subq, sites
                        WHERE sites.site = subq.site
                        ORDER BY sites.site;
                    """

    with sql_engine.connect() as conn:
    # create the dataframes from the sql query
        mercury_df = pd.read_sql_query(sql_data_query, conn)
    
    mercury_df_sorted=mercury_df.sort_values(by='avg_concentration')
    
    # plot the means and std's
    plt.figure(figsize=(80, 10))
    plt.errorbar(mercury_df_sorted['site_name_full'], mercury_df_sorted['avg_concentration'], yerr=mercury_df_sorted['stdev_concentration'], fmt='o', capsize=5, capthick=2, elinewidth=1.5, markersize=4)
    plt.xticks(rotation=90)
    # ipcc_site_mean_df.plot.scatter('site', 'mean', yerr='std', capsize=3, title=('Mean site TGM/GEM for '+ipcc_region),xlabel='site',ylabel='Concentration (ngm$\mathregular{^3}$)',rot=90)
    plt.title('Mean site TGM/GEM For All sites ')
    plt.xlabel('site')
    plt.ylabel('Concentration (ngm$\mathregular{^-3}$)')
    plt.ylim(-10,100)
    plt.tight_layout()
    plt.savefig('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Results and Plots/mean_concentration_global.png')
    plt.show(block=True)

################## averaging function by ipcc_region #########################

def ipcc_site_average(sql_engine,ipcc_regions_list):
    
    # for ipcc_region in ipcc_analysis_df['ipcc_region']:
    for ipcc_region in ipcc_regions_list:#ipcc_analysis_df.index:
        print (ipcc_region)

        #define a sql query to grab the concentration from a particular site for a particular species from 2010 on
        sql_data_query = """
                    SET TIME ZONE 'GMT';
                    SELECT CONCAT( subq.site, ' (', sites.country_code, ')') AS site_name_full
                        , subq.avg_concentration
                        , subq.stdev_concentration
                    FROM (
                        SELECT site ,avg(concentration)::float AS avg_concentration ,stddev(concentration)::float AS stdev_concentration 
                        FROM all__hgee_v2 GROUP BY site
                        ) AS subq, sites
                        WHERE sites.site = subq.site
                        and ipcc_region = '{}'
                        ORDER BY sites.site;
                """.format(ipcc_region)    

        with sql_engine.connect() as conn:
        # create the dataframes from the sql query
            mercury_df = pd.read_sql_query(sql_data_query, conn)

    if mercury_df.shape[0]!=0:
        # assign the country code to the dataframe

        # plot the means and std's
        plt.figure(figsize=(20, 10))
        plt.errorbar(mercury_df['site_name_full'], mercury_df['avg_concentration'], yerr=mercury_df['stdev_concentration'], fmt='o', capsize=5, capthick=2, elinewidth=1.5, markersize=4)
        plt.xticks(rotation=90)
        # ipcc_site_mean_df.plot.scatter('site', 'mean', yerr='std', capsize=3, title=('Mean site TGM/GEM for '+ipcc_region),xlabel='site',ylabel='Concentration (ngm$\mathregular{^3}$)',rot=90)
        plt.title('Mean site TGM/GEM For '+ipcc_region)
        plt.xlabel('site')
        plt.ylabel('Concentration (ngm$\mathregular{^-3}$)')
        plt.ylim(-10,100)
        plt.tight_layout()
        plt.savefig('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Results and Plots/mean_concentration_'+ipcc_region+'.png')
        plt.show(block=True)

############################# monthly mean function for trend analysis and such ##########

def monthly_mean(sql_engine,site_list):
        # for ipcc_region in ipcc_analysis_df['ipcc_region']:
    for site in site_list:#ipcc_analysis_df.index:
        print (site)

        #define a sql query to grab the concentration from a particular site for a particular species from 2010 on
        sql_data_query = """
                    SET TIME ZONE 'GMT';
                    select datetime, concentration from all__hgee_v2
                    where species in ['TGM','GEM'] and site  = '{}'
                    order by datetime
                """.format(site)    

        with sql_engine.connect() as conn:
        # create the dataframes from the sql query
            mercury_df = pd.read_sql_query(sql_data_query, conn)
            month_list=[] # initialize a blank month list
            for i in range(1,13):
                month_list.append(calendar.month_name[i][:3])

            # create a blank dataframe with 12 column months and 31 calendar days as the index
            master_monthly_df=pd.DataFrame(columns=month_list)

            for i, month in enumerate(month_list):
                print (month)
                datetime_month=i+1
                # print (mercury_df['datetime'].dt.month==datetime_month)
                monthly_df_index = mercury_df.loc[mercury_df['datetime'].dt.month==datetime_month].index
                # print (mercury_df.loc[monthly_df_index,'concentration'].reset_index(drop=True))
                master_monthly_df[month]=mercury_df.loc[monthly_df_index,'concentration'].reset_index(drop=True)

        # plot the means and std's
        plt.figure(figsize=(20, 10))
        master_monthly_df.boxplot()    
        plt.title('site TGM/GEM Boxplot For '+ipcc_region)
        plt.xlabel('Month')
        plt.ylabel('Concentration (ngm$\mathregular{^-3}$)')
        plt.tight_layout()
        plt.savefig('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Results and Plots/monthly_box_plot_'+site+'.png')
        plt.show(block=True)

############################## extraction of frequency information from the minamata format files ##############
def site_frequency_test():
    # initialize a frequency dataframe for tracking those with frequency information
    frequency_df = pd.DataFrame(columns=['frequency'])
    frequency_df.index.name = 'site'
    folders='\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Data/*'
    folder_list=glob.glob(folders)
    folder_list.remove('\\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Data\\Passives') # grab all the folders 
    # print (folder_list)
    for folder in folder_list: # for each folder grab all the files
        print (folder)
        folder_path=folder+'/air_data/minamata_format/*.csv'
        file_list=glob.glob(folder_path)        
        for file in file_list: # for each file in the list
            if len(file)>260:
                print (len(file))
                print (file)
                continue
            file_metadata_df=pd.read_csv(file,index_col=0,nrows=60,usecols=range(3))
            # print (file_metadata_df.columns)
            site_name=file_metadata_df.loc['*site IDENTIFICATION','Unnamed: 1']
            data_frequency=file_metadata_df.loc['*SAMPLING FREQUENCY OF DATA IN THIS FILE','Unnamed: 1']
            # check to see if that site is already included
            if site_name not in frequency_df.index:
                frequency_df.loc[site_name,'frequency']=data_frequency
    print (frequency_df)
    frequency_df.to_csv('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Results and Plots/frequency_info.csv')

########################### function to insert frequency data into the table ###################################
def freq_insert(sql_engine):
    # load the csv file into a dataframe
    freq_df=pd.read_csv('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Results and Plots/all_sites_frequency.csv')
    print (freq_df.head)
    # Connect and execute update per row
    with sql_engine.begin() as conn:
        for _, row in freq_df.iterrows():
            print (row["site"],row["frequency"])
            conn.execute(
                text("""
                    UPDATE sites
                    SET freq = :frequency
                    WHERE site = :site  
                """),
                {"site": row["site"], "frequency": row["frequency"]}
            )

def coverage_calculator(sql_engine):
    # define an sql query that grabs the duration from the hgee_active table and frequency from the sites table
    sql_data_query = """
                        SELECT 
                            s.site, 
                            s.freq, 
                            h.min_dt, 
                            h.max_dt,
                            h.count
                        FROM sites s
                        JOIN (
                            SELECT 
                                site, 
                                MIN(datetime) AS min_dt, 
                                MAX(datetime) AS max_dt,
                                COUNT(*) AS count
                            FROM hgee_active
                            WHERE matrix = 'air'
                            GROUP BY site
                        ) h ON s.site = h.site
                        WHERE h.min_dt > '2000-01-01'
                        AND h.max_dt > '2015-01-01';        """
                            
    
    with sql_engine.begin() as conn:
        # put the results into a dataframe
        freq_df = pd.read_sql_query(sql_data_query, conn)

    freq_df['freq'] = pd.to_timedelta(freq_df['freq'])
    freq_df['perfect_count'] = (freq_df['max_dt'] - freq_df['min_dt']) / freq_df['freq']
    freq_df['percent_coverage'] = freq_df['count']/freq_df['perfect_count']*100
    print(freq_df.loc[freq_df['percent_coverage'] < 75, ['site', 'percent_coverage']])
    # print (freq_df.loc[:,['site', 'percent_coverage']])
    
############################ gantt chart for data availability ###################################
def gantt_plotter(sql_engine):
    # set the psql query
    sql_data_query = """
                set time zone GMT;
                select distinct on (site) site, country, min(datetime) as start_dt, max(datetime) as end_dt,species from hgee_active where datetime > '2000-01-01' GROUP BY (site,country,species); 
            """

    with sql_engine.connect() as conn:
    # create the dataframes from the sql query
        gantt_mercury_tracking_dataframe = pd.read_sql_query(sql_data_query, conn)

    
    # grab the passives (for now) from teh big passives file
    passives_gantt_tracker_df=pd.read_csv('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Data/Passives/data_files/global_passives.csv',usecols=[1,2,3,4])

    passives_gantt_tracker_df['start_dt']=pd.to_datetime(passives_gantt_tracker_df['start_dt'])
    passives_gantt_tracker_df['end_dt']=pd.to_datetime(passives_gantt_tracker_df['end_dt'])

    # Create new rows to add to the Gantt tracking dataframe
    passive_rows = []

    for site in passives_gantt_tracker_df['site'].unique():
        site_df = passives_gantt_tracker_df[passives_gantt_tracker_df['site'] == site]
        start_date = site_df['start_dt'].min()
        end_date = site_df['end_dt'].max()
        country = site_df['country'].iloc[0]  # Extract single value
        passive_species = 'TPM'

        # only use samples that spanned more than one 'season'
        if len(site_df['start_dt'])>1:
            passive_rows.append({
                'site': site,
                'country': country,
                'start_dt': start_date,
                'end_dt': end_date,
                'species': passive_species
            })

    # Create a DataFrame from the new rows and append to the existing Gantt dataframe
    passive_df = pd.DataFrame(passive_rows)
    gantt_mercury_tracking_dataframe = pd.concat([gantt_mercury_tracking_dataframe, passive_df], ignore_index=True)

    # concatenate the site and country to one lable
    gantt_mercury_tracking_dataframe['site'] = gantt_mercury_tracking_dataframe['site']+' '+gantt_mercury_tracking_dataframe['country']
    gantt_mercury_tracking_dataframe.drop(columns=['country'], inplace=True)
    
    # set the site as the index
    gantt_mercury_tracking_dataframe.set_index('site',drop=True, inplace=True)

    
    # loop through each species
    species_list=gantt_mercury_tracking_dataframe['species'].unique()
    for species in species_list:
        print (species)
        sub_df = gantt_mercury_tracking_dataframe[gantt_mercury_tracking_dataframe['species'] == species]
        print (sub_df.index)

        # sort the df by end_dt
        sub_df.sort_values(by=['end_dt'], inplace=True)        
        
        if species=='PBM':
            fig_size=(15,5)
        elif species=='GEM':
            fig_size=(15,15)
        elif species=='GOM':
            fig_size=(15,5)
        elif species=='total_mercury':
            fig_size=(15,30)
        elif species=='TGM':
            fig_size=(15,15)
        elif species=='TPM':
            fig_size=(15,40)                

        fig,ax = plt.subplots(figsize=fig_size)
        ax.xaxis_date()
        plt.hlines(sub_df.index, md.date2num(sub_df['start_dt']), md.date2num(sub_df['end_dt']), )
        plt.xticks(fontsize=20)
        plt.xlabel('Date',fontsize=20)
        plt.ylabel('Station',fontsize=20)
        plt.title('Mercury Measurement Time Span by Station for: '+species,fontsize=20)
        plt.grid(axis="x")
        # plt.legend(labels=subset_df.index)
        # plt.show()
        fig.tight_layout()
        plt.savefig('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Results and Plots/gantt-plot_'+species+'.png')
        plt.close()
   
def monthly_timeseries(sql_engine):
    # set the sql query
    sql_data_query = """
        SET TIME ZONE 'GMT'; 
        SELECT 
            site, ipcc_region,
            date_trunc('month', datetime) AS month,
            AVG(concentration) AS monthly_mean
        FROM 
            hgee_active
        WHERE 
            species IN ('TGM', 'GEM')
            AND datetime > '2000-01-01'
        GROUP BY 
            site, ipcc_region, date_trunc('month', datetime)
        ORDER BY 
            site, month;
    """

    with sql_engine.connect() as conn:
        mercury_df = pd.read_sql_query(sql_data_query, conn)

    # Replace bad values with NaN
    mercury_df.replace({'monthly_mean': {999: np.nan, 999.999: np.nan}}, inplace=True)

    # Ensure datetime
    mercury_df['month'] = pd.to_datetime(mercury_df['month'])

    # Get unique regions
    regions = mercury_df['ipcc_region'].dropna().unique()

    for region in regions:
        region_df = mercury_df[mercury_df['ipcc_region'] == region]

        fig, ax = plt.subplots(figsize=(14, 5))

        for site, group in region_df.groupby('site'):
            ax.plot(group['month'], group['monthly_mean'], label=site)

        ax.set_title(f'Monthly Mean Mercury Concentration – {region}')
        ax.set_xlabel('Date')
        ax.set_ylabel('Concentration')
        ax.legend(title='Site', loc='center left', bbox_to_anchor=(1.01, 0.5))

        plt.tight_layout(rect=[0, 0, 0.85, 1])

        # Sanitize region name for filenames
        safe_region = region.replace(" ", "_").replace("/", "_")

        # Define full output path
        output_path = f"\\\\econm3hwvfsp008.ncr.int.ec.gc.ca\\arqp_data\\Projects\\OnGoing\\Mercury\\HGEE-Minamata\\Results and Plots\\global_mean_timeseries_{safe_region}.png"
        
        plt.savefig(output_path)
        plt.close(fig)  # close the figure to free memory

# a global concentration plotter
def global_plotter(sql_engine):
    # Run SQL and load into pandas
    sql_query = """
    SELECT 
        site, 
        latdecd, 
        londecd, 
        AVG(concentration) AS mean_concentration
    FROM 
        hgee_active
    WHERE 
        species in ('TGM', 'GEM')
    and
        datetime > '2020-01-01'    GROUP BY 
        site, latdecd, londecd;
    """

    with sql_engine.connect() as conn:
        df = pd.read_sql_query(sql_query, conn)

    # Replace bad values
    df['mean_concentration'] = df['mean_concentration'].replace([999, 999.999], pd.NA)

    # Drop rows with NA
    df = df.dropna(subset=['mean_concentration', 'latdecd', 'londecd'])

    # Convert degrees to radians for Winkel-Tripel projection
    df['lon_rad'] = np.radians(df['londecd'])
    df['lat_rad'] = np.radians(df['latdecd'])

    # Create geometry column for geopandas
    gdf = gp.GeoDataFrame(df, 
                        geometry=gp.points_from_xy(df['londecd'], df['latdecd']),
                        crs='EPSG:4326')  # WGS84

    # Step 4: Reproject to Winkel-Tripel (ESRI:54042)
    gdf_winkel = gdf.to_crs('ESRI:54042')

    # Load world map
    world = gp.read_file("https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip")
    world = world.to_crs('ESRI:54042')

    # Get value range for color normalization
    vmin = gdf_winkel['mean_concentration'].min()
    vmax = gdf_winkel['mean_concentration'].max()
    norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)
    cmap = plt.get_cmap('turbo')

    # Start plotting
    fig, ax = plt.subplots(figsize=(16, 10))

    # Plot world polygons
    world.plot(ax=ax, color='peru', edgecolor='dimgray', linewidth=0.5, zorder=2)

    # Plot mercury data points
    gdf_winkel.plot(
        ax=ax,
        column='mean_concentration',
        cmap=cmap,
        norm=norm,
        markersize=40,
        edgecolor='k',
        linewidth=0.2,
        alpha=0.9,
        legend=False
    )

    # Set axis limits to world extent in Winkel-Tripel
    ax.set_xlim(world.total_bounds[[0, 2]])
    ax.set_ylim(world.total_bounds[[1, 3]])
    ax.set_axis_off()

    # Add colorbar
    sm = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)
    sm._A = []
    cbar = fig.colorbar(sm, ax=ax, orientation='vertical', fraction=0.03, pad=0.04, aspect=20)
    cbar.ax.set_title("Hg\n[ng/m³]", fontsize=10)
    cbar.ax.tick_params(labelsize=8)

    # Reduce vertical extent of colorbar
    scale_box = cbar.ax.get_position()
    # Compute new height
    new_height = scale_box.height * 0.5

    # Adjust y-position to keep it centered
    new_y0 = scale_box.y0 + (scale_box.height - new_height) / 2

    # Set new position
    cbar.ax.set_position([scale_box.x0, new_y0, scale_box.width, new_height])

    # Final title
    ax.set_title('Global Mean Active TGM/GEM Concentrations (Post-2020)', fontsize=14)
    # plt.tight_layout()
    # plt.subplots_adjust(left=0.1, right=0.9, bottom=0.2, top=0.8, )
    plt.show()
    plt.savefig('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Results and Plots/global_averages.png')

def time_delta_by_site(sql_engine):
    # grab the sites and frequencies from the sites table
    site_sql_query = """
                    SELECT distinct sites.site, sites.freq
                    FROM sites
                    JOIN hgee_active ON sites.site = hgee_active.site
                    WHERE sites.freq IS NOT NULL
                    AND hgee_active.datetime > '1999-12-31'
                    and hgee_active.species in ('TGM','GEM')
                    order by site;
                    """

    # grab the list of sites and frequencies
    with sql_engine.connect() as conn:
        sites_df = pd.read_sql_query(site_sql_query, conn)
        
    # set the index to the sites
    sites_df.set_index(sites_df['site'], drop=True, inplace=True)

    compliance_df = pd.DataFrame(index=sites_df.index,columns=['total_time','non_compliance_time','compliance_fraction'])
    
    for site in sites_df.index:
        print (site)

         # grab the datetimes from each site
        sql_query = """
                    select datetime from hgee_active 
                    where concentration is not null
                    and datetime > '1999-12-31'
                    and species in ('TGM','GEM')
                    and site = '{}'
                    order by datetime;
                    """.format(site)
        with sql_engine.connect() as conn:
            datetimes_df = pd.read_sql_query(sql_query, conn)
        
        # set the datetime column to a datetime column
        datetimes_df['datetime'] = pd.to_datetime(datetimes_df['datetime'])

        # calculate the delta_t between measurements
        datetimes_df['delta_t'] = datetimes_df['datetime'].diff()

        # Convert to hours (or minutes/seconds if desired)
        datetimes_df['delta_t_hours'] = datetimes_df['delta_t'].dt.total_seconds() / 3600

        # Drop NaNs from the first row (diff produces NaN at index 0)
        delta_hours = datetimes_df['delta_t_hours'].dropna()

        # Compute the mode of delta_hours
        mode_delta = stats.mode(delta_hours, keepdims=True)[0][0]

        # Set tolerance as a fraction of the mode (15%)
        tolerance = 0.15

        # Identify values outside the tolerance range
        non_mode_mask = np.abs(delta_hours - mode_delta) > tolerance*mode_delta

        # Get the indices of these "non-mode" delta_t values
        non_mode_indices = delta_hours[non_mode_mask].index

        # Filter original DataFrame to see those rows
        deviations_df = datetimes_df.loc[non_mode_indices]

        # sum up the 'non-compliance time'
        non_compliance_time=datetimes_df.loc[non_mode_indices,'delta_t_hours'].sum()

        total_time = (datetimes_df.loc[datetimes_df.index[-1], 'datetime'] - datetimes_df.loc[datetimes_df.index[0], 'datetime']).total_seconds() / 3600

        # print (total_time)
        # print (non_compliance_time)

        compliance_df.loc[site,['total_time','non_compliance_time','compliance_fraction']] = total_time,non_compliance_time,non_compliance_time/total_time

        # # Plot histogram
        # plt.figure(figsize=(10, 6))
        # n, bins, patches = plt.hist(delta_hours, bins=50, color='skyblue', edgecolor='black')

        # plt.title('Histogram of Δt (Time Between Measurements)')
        # plt.xlabel('Δt (hours)')
        # plt.ylabel('Frequency')
        # plt.grid(True)
        # plt.tight_layout()
        # plt.show()
        # plt.savefig('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Results and Plots/delta_t_'+site+'.png')

    # print (compliance_df)

    # save the compliance dataframe
    compliance_df.to_csv('\\\econm3hwvfsp008.ncr.int.ec.gc.ca/arqp_data/Projects/OnGoing/Mercury/HGEE-Minamata/Results and Plots/measurement_frequency_compliance.csv')

# # run the global average
# global_site_average(sql_engine)

# # run the ipcc average
# ipcc_site_average(sql_engine,ipcc_regions_list)

# run the ipcc monthly box plot
# monthly_mean(sql_engine,ipcc_regions_list)

# run the frequency test
#site_frequency_test()

# run the frequency table insert
# freq_insert(sql_engine)

# run the coverage calculator
# coverage_calculator(sql_engine)

# run the gantt plotter
# gantt_plotter(sql_engine)

# run the monthly timeseries
# monthly_timeseries(sql_engine)

# run the global plotter
# global_plotter(sql_engine)

# run a histogram of the typical delta t for each site
time_delta_by_site(sql_engine)